{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import cv2\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2.aruco as aruco\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "video_path = 'buoy_video.mp4'  # Replace with your video path\n",
    "output_stabilized = 'stabilized_video3.mp4'  # Output path for stabilized video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stabilization\n",
    "\n",
    "by: https://github.com/krutikabapat/Video-Stabilization-using-OpenCV/blob/master/video_stabilization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430\n",
      "Width: 1440\n",
      "Height: 1080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x47504a4d/'MJPG' is not supported with codec id 7 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "SMOOTHING_RADIUS = 50\n",
    "\n",
    "def movingAverage(curve, radius): \n",
    "    window_size = 2 * radius + 1\n",
    "    f = np.ones(window_size) / window_size \n",
    "    curve_pad = np.lib.pad(curve, (radius, radius), 'edge') \n",
    "    curve_smoothed = np.convolve(curve_pad, f, mode='same') \n",
    "    curve_smoothed = curve_smoothed[radius:-radius]\n",
    "    return curve_smoothed \n",
    "\n",
    "def smooth(trajectory): \n",
    "    smoothed_trajectory = np.copy(trajectory) \n",
    "    for i in range(3):\n",
    "        smoothed_trajectory[:, i] = movingAverage(trajectory[:, i], radius=SMOOTHING_RADIUS)\n",
    "    return smoothed_trajectory\n",
    "\n",
    "def fixBorder(frame):\n",
    "    s = frame.shape\n",
    "    T = cv2.getRotationMatrix2D((s[1]/2, s[0]/2), 0, 1.04)\n",
    "    frame = cv2.warpAffine(frame, T, (s[1], s[0]))\n",
    "    return frame\n",
    "\n",
    "# Read input video\n",
    "cp = cv2.VideoCapture(video_path)\n",
    "\n",
    "n_frames = int(cp.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(n_frames)\n",
    "\n",
    "width = int(cp.get(cv2.CAP_PROP_FRAME_WIDTH)) \n",
    "height = int(cp.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "print(\"Width:\", width)\n",
    "print(\"Height:\", height)\n",
    "\n",
    "fps = cp.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "out = cv2.VideoWriter(output_stabilized, fourcc, fps, (width, height))\n",
    "\n",
    "_, prev = cp.read()\n",
    "prev_gray = cv2.cvtColor(prev, cv2.COLOR_BGR2GRAY)\n",
    "transforms = np.zeros((n_frames - 1, 3), np.float32) \n",
    "\n",
    "for i in range(n_frames - 2):\n",
    "    prev_pts = cv2.goodFeaturesToTrack(prev_gray, maxCorners=200, qualityLevel=0.01, minDistance=30, blockSize=3)\n",
    "    succ, curr = cp.read()\n",
    "\n",
    "    if not succ:\n",
    "        break\n",
    "\n",
    "    curr_gray = cv2.cvtColor(curr, cv2.COLOR_BGR2GRAY)\n",
    "    curr_pts, status, err = cv2.calcOpticalFlowPyrLK(prev_gray, curr_gray, prev_pts, None)\n",
    "\n",
    "    idx = np.where(status == 1)[0]\n",
    "    prev_pts = prev_pts[idx]\n",
    "    curr_pts = curr_pts[idx]\n",
    "    assert prev_pts.shape == curr_pts.shape \n",
    "\n",
    "    m, inliers = cv2.estimateAffine2D(prev_pts, curr_pts)\n",
    "    if m is None:\n",
    "        print(\"Could not estimate affine transformation\")\n",
    "        continue\n",
    "\n",
    "    dx = m[0, 2]\n",
    "    dy = m[1, 2]\n",
    "    da = np.arctan2(m[1, 0], m[0, 0])\n",
    "    transforms[i] = [dx, dy, da] \n",
    "    prev_gray = curr_gray\n",
    "\n",
    "trajectory = np.cumsum(transforms, axis=0) \n",
    "smoothed_trajectory = smooth(trajectory)\n",
    "difference = smoothed_trajectory - trajectory\n",
    "transforms_smooth = transforms + difference\n",
    "\n",
    "cp.set(cv2.CAP_PROP_POS_FRAMES, 0) \n",
    "for i in range(n_frames - 2):\n",
    "    success, frame = cp.read() \n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    dx = transforms_smooth[i, 0]\n",
    "    dy = transforms_smooth[i, 1]\n",
    "    da = transforms_smooth[i, 2]\n",
    "\n",
    "    m = np.zeros((2, 3), np.float32)\n",
    "    m[0, 0] = np.cos(da)\n",
    "    m[0, 1] = -np.sin(da)\n",
    "    m[1, 0] = np.sin(da)\n",
    "    m[1, 1] = np.cos(da)\n",
    "    m[0, 2] = dx\n",
    "    m[1, 2] = dy\n",
    "\n",
    "    frame_stabilized = cv2.warpAffine(frame, m, (width, height))\n",
    "    frame_stabilized = fixBorder(frame_stabilized) \n",
    "\n",
    "    frame_out = cv2.hconcat([frame, frame_stabilized])\n",
    "    \n",
    "    if frame_out.shape[1] > 1920: \n",
    "        frame_out = cv2.resize(frame_out, (int(frame_out.shape[1] / 2), int(frame_out.shape[0] / 2)))\n",
    "    \n",
    "    cv2.imshow(\"Before and After\", frame_out)\n",
    "    cv2.waitKey(10)\n",
    "    out.write(frame_stabilized)\n",
    "\n",
    "cp.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432\n",
      "Width: 1440\n",
      "Height: 1080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x47504a4d/'MJPG' is not supported with codec id 7 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "SMOOTHING_RADIUS = 50\n",
    "\n",
    "def movingAverage(curve, radius): \n",
    "    window_size = 2 * radius + 1\n",
    "    f = np.ones(window_size) / window_size \n",
    "    curve_pad = np.lib.pad(curve, (radius, radius), 'edge') \n",
    "    curve_smoothed = np.convolve(curve_pad, f, mode='same') \n",
    "    curve_smoothed = curve_smoothed[radius:-radius]\n",
    "    return curve_smoothed \n",
    "\n",
    "def smooth(trajectory): \n",
    "    smoothed_trajectory = np.copy(trajectory) \n",
    "    for i in range(3):\n",
    "        smoothed_trajectory[:, i] = movingAverage(trajectory[:, i], radius=SMOOTHING_RADIUS)\n",
    "    return smoothed_trajectory\n",
    "\n",
    "def fixBorder(frame):\n",
    "    s = frame.shape\n",
    "    T = cv2.getRotationMatrix2D((s[1]/2, s[0]/2), 0, 1.04)\n",
    "    frame = cv2.warpAffine(frame, T, (s[1], s[0]))\n",
    "    return frame\n",
    "\n",
    "def detect_horizon(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, 50, 150, apertureSize=3)\n",
    "    lines = cv2.HoughLines(edges, 1, np.pi / 180, 150)\n",
    "    if lines is not None:\n",
    "        for rho, theta in lines[:, 0]:\n",
    "            if 0.9 < theta < 2.2:  # Only keep lines close to horizontal\n",
    "                return theta\n",
    "    return None\n",
    "\n",
    "# Read input video\n",
    "cp = cv2.VideoCapture(video_path)\n",
    "\n",
    "n_frames = int(cp.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(n_frames)\n",
    "\n",
    "width = int(cp.get(cv2.CAP_PROP_FRAME_WIDTH)) \n",
    "height = int(cp.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "print(\"Width:\", width)\n",
    "print(\"Height:\", height)\n",
    "\n",
    "fps = cp.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "out = cv2.VideoWriter(output_stabilized, fourcc, fps, (width, height))\n",
    "\n",
    "_, prev = cp.read()\n",
    "prev_gray = cv2.cvtColor(prev, cv2.COLOR_BGR2GRAY)\n",
    "transforms = np.zeros((n_frames - 1, 3), np.float32) \n",
    "\n",
    "for i in range(n_frames - 2):\n",
    "    prev_pts = cv2.goodFeaturesToTrack(prev_gray, maxCorners=200, qualityLevel=0.01, minDistance=30, blockSize=3)\n",
    "    succ, curr = cp.read()\n",
    "\n",
    "    if not succ:\n",
    "        break\n",
    "\n",
    "    curr_gray = cv2.cvtColor(curr, cv2.COLOR_BGR2GRAY)\n",
    "    curr_pts, status, err = cv2.calcOpticalFlowPyrLK(prev_gray, curr_gray, prev_pts, None)\n",
    "\n",
    "    idx = np.where(status == 1)[0]\n",
    "    prev_pts = prev_pts[idx]\n",
    "    curr_pts = curr_pts[idx]\n",
    "    assert prev_pts.shape == curr_pts.shape \n",
    "\n",
    "    m, inliers = cv2.estimateAffine2D(prev_pts, curr_pts)\n",
    "    if m is None:\n",
    "        print(\"Could not estimate affine transformation\")\n",
    "        continue\n",
    "\n",
    "    dx = m[0, 2]\n",
    "    dy = m[1, 2]\n",
    "    da = np.arctan2(m[1, 0], m[0, 0])\n",
    "    \n",
    "    # Horizon-based correction\n",
    "    horizon_angle = detect_horizon(curr)\n",
    "    if horizon_angle is not None:\n",
    "        correction_angle = horizon_angle - np.pi / 2\n",
    "        da += correction_angle  # Adjust rotation based on horizon\n",
    "\n",
    "    transforms[i] = [dx, dy, da] \n",
    "    prev_gray = curr_gray\n",
    "\n",
    "trajectory = np.cumsum(transforms, axis=0) \n",
    "smoothed_trajectory = smooth(trajectory)\n",
    "difference = smoothed_trajectory - trajectory\n",
    "transforms_smooth = transforms + difference\n",
    "\n",
    "cp.set(cv2.CAP_PROP_POS_FRAMES, 0) \n",
    "for i in range(n_frames - 2):\n",
    "    success, frame = cp.read() \n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    dx = transforms_smooth[i, 0]\n",
    "    dy = transforms_smooth[i, 1]\n",
    "    da = transforms_smooth[i, 2]\n",
    "\n",
    "    m = np.zeros((2, 3), np.float32)\n",
    "    m[0, 0] = np.cos(da)\n",
    "    m[0, 1] = -np.sin(da)\n",
    "    m[1, 0] = np.sin(da)\n",
    "    m[1, 1] = np.cos(da)\n",
    "    m[0, 2] = dx\n",
    "    m[1, 2] = dy\n",
    "\n",
    "    frame_stabilized = cv2.warpAffine(frame, m, (width, height))\n",
    "    frame_stabilized = fixBorder(frame_stabilized) \n",
    "\n",
    "    frame_out = cv2.hconcat([frame, frame_stabilized])\n",
    "    \n",
    "    if frame_out.shape[1] > 1920: \n",
    "        frame_out = cv2.resize(frame_out, (int(frame_out.shape[1] / 2), int(frame_out.shape[0] / 2)))\n",
    "    \n",
    "    cv2.imshow(\"Before and After\", frame_out)\n",
    "    cv2.waitKey(10)\n",
    "    out.write(frame_stabilized)\n",
    "\n",
    "cp.release()\n",
    "out.release()\n",
    "cv2.destroyWindow(\"Before and After\")\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x47504a4d/'MJPG' is not supported with codec id 7 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 74\u001b[0m\n\u001b[1;32m     71\u001b[0m     out\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m     72\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mdestroyWindow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBefore and After\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 74\u001b[0m \u001b[43mstabilize_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_stabilized\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 43\u001b[0m, in \u001b[0;36mstabilize_video\u001b[0;34m(video_path, output_path)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Detect the horizon and draw it on the frame\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m frame_with_horizon, horizon_angle \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_horizon_and_draw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# If a horizon angle is detected, use it to correct the frame's rotation\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m horizon_angle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# Calculate the rotation needed to align the horizon\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m, in \u001b[0;36mdetect_horizon_and_draw\u001b[0;34m(frame)\u001b[0m\n\u001b[1;32m      5\u001b[0m edges \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mCanny(gray, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m100\u001b[39m, apertureSize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Use Hough line transform to detect lines\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m lines \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHoughLines\u001b[49m\u001b[43m(\u001b[49m\u001b[43medges\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m180\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m horizon_angle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lines \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def detect_horizon_and_draw(frame):\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # Use Canny edge detection\n",
    "    edges = cv2.Canny(gray, 10, 100, apertureSize=7)\n",
    "    # Use Hough line transform to detect lines\n",
    "    lines = cv2.HoughLines(edges, 1, np.pi / 180, 150)\n",
    "    \n",
    "    horizon_angle = None\n",
    "    if lines is not None:\n",
    "        for rho, theta in lines[:, 0]:\n",
    "            # Only consider lines close to horizontal (angle between 0.9 and 2.2 radians)\n",
    "            if 0.9 < theta < 2.2:\n",
    "                horizon_angle = theta\n",
    "                # Draw the detected line on the frame\n",
    "                a = np.cos(theta)\n",
    "                b = np.sin(theta)\n",
    "                x0 = a * rho\n",
    "                y0 = b * rho\n",
    "                x1 = int(x0 + 1000 * (-b))\n",
    "                y1 = int(y0 + 1000 * a)\n",
    "                x2 = int(x0 - 1000 * (-b))\n",
    "                y2 = int(y0 - 1000 * a)\n",
    "                cv2.line(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                break  # Use the first detected horizon line\n",
    "\n",
    "    return frame, horizon_angle\n",
    "\n",
    "def stabilize_video(video_path, output_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Detect the horizon and draw it on the frame\n",
    "        frame_with_horizon, horizon_angle = detect_horizon_and_draw(frame)\n",
    "        \n",
    "        # If a horizon angle is detected, use it to correct the frame's rotation\n",
    "        if horizon_angle is not None:\n",
    "            # Calculate the rotation needed to align the horizon\n",
    "            correction_angle = (horizon_angle - np.pi / 2) * (180 / np.pi)\n",
    "            # Get the rotation matrix\n",
    "            center = (width // 2, height // 2)\n",
    "            rotation_matrix = cv2.getRotationMatrix2D(center, correction_angle, 1)\n",
    "            # Rotate the frame to stabilize the horizon\n",
    "            frame_with_horizon = cv2.warpAffine(frame_with_horizon, rotation_matrix, (width, height))\n",
    "\n",
    "        # Write the stabilized frame to the output video\n",
    "        out.write(frame_with_horizon)\n",
    "        \n",
    "        # Display the frame\n",
    "        frame_out = cv2.hconcat([frame, frame_with_horizon])\n",
    "    \n",
    "        if frame_out.shape[1] > 1920: \n",
    "            frame_out = cv2.resize(frame_out, (int(frame_out.shape[1] / 2), int(frame_out.shape[0] / 2)))\n",
    "        \n",
    "        cv2.imshow(\"Before and After\", frame_out)\n",
    "\n",
    "        # cv2.imshow(\"Stabilized Video with Horizon\", frame_with_horizon)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyWindow(\"Before and After\")\n",
    "\n",
    "stabilize_video(video_path, output_stabilized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def detect_horizon_line(image_grayscaled):\n",
    "    \"\"\"Detect the horizon's starting and ending points in the given image.\n",
    "\n",
    "    The horizon line is detected by applying Otsu's threshold method to\n",
    "    separate the sky from the remainder of the image.\n",
    "    \"\"\"\n",
    "    msg = ('`image_grayscaled` should be a grayscale, 2-dimensional image '\n",
    "           'of shape (height, width).')\n",
    "    assert image_grayscaled.ndim == 2, msg\n",
    "\n",
    "    # Apply Gaussian blur to smooth the image\n",
    "    image_blurred = cv2.GaussianBlur(image_grayscaled, ksize=(3, 3), sigmaX=0)\n",
    "    \n",
    "    # Apply Otsu's thresholding\n",
    "    _, image_thresholded = cv2.threshold(\n",
    "        image_blurred, thresh=0, maxval=1,\n",
    "        type=cv2.THRESH_BINARY + cv2.THRESH_OTSU\n",
    "    )\n",
    "    image_thresholded = image_thresholded - 1\n",
    "    \n",
    "    # Apply morphological closing to fill gaps\n",
    "    image_closed = cv2.morphologyEx(image_thresholded, cv2.MORPH_CLOSE,\n",
    "                                    kernel=np.ones((9, 9), np.uint8))\n",
    "    \n",
    "    horizon_x1 = 0\n",
    "    horizon_x2 = image_grayscaled.shape[1] - 1\n",
    "    # Find the maximum y-coordinate where the horizon line is detected\n",
    "    horizon_y1 = max(np.where(image_closed[:, horizon_x1] == 0)[0])\n",
    "    horizon_y2 = max(np.where(image_closed[:, horizon_x2] == 0)[0])\n",
    "\n",
    "    return horizon_x1, horizon_x2, horizon_y1, horizon_y2\n",
    "\n",
    "# Load the video\n",
    "# video_path = 'path_to_your_video.mp4'  # Replace with your video path\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect the horizon line\n",
    "    try:\n",
    "        horizon_x1, horizon_x2, horizon_y1, horizon_y2 = detect_horizon_line(gray_frame)\n",
    "        \n",
    "        # Draw the horizon line on the frame\n",
    "        frame_with_horizon = frame.copy()\n",
    "        cv2.line(frame_with_horizon, (horizon_x1, horizon_y1), (horizon_x2, horizon_y2), (0, 0, 255), 2)\n",
    "\n",
    "        # Concatenate the original frame and the frame with the horizon line\n",
    "        combined_frame = cv2.hconcat([frame, frame_with_horizon])\n",
    "\n",
    "        # Display the combined frame\n",
    "        cv2.imshow(\"Before and After\", combined_frame)\n",
    "    except Exception as e:\n",
    "        print(\"Error detecting horizon:\", e)\n",
    "        continue\n",
    "\n",
    "    # Break on 'q' key press\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Detect the horizon line\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     horizon_x1, horizon_x2, horizon_y1, horizon_y2 \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_horizon_line_combined\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# Draw the horizon line on the frame\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     frame_with_horizon \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mcopy()\n",
      "Cell \u001b[0;32mIn[22], line 29\u001b[0m, in \u001b[0;36mdetect_horizon_line_combined\u001b[0;34m(image_grayscaled)\u001b[0m\n\u001b[1;32m     26\u001b[0m             horizon_y_candidates\u001b[38;5;241m.\u001b[39mappend((y1 \u001b[38;5;241m+\u001b[39m y2) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Step 4: Calculate the Otsu-based horizon line position\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m horizon_y_otsu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_thresholded\u001b[49m\u001b[43m[\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m height \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Step 5: Combine the methods\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m horizon_y_candidates:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def detect_horizon_line_combined(image_grayscaled):\n",
    "    # Step 1: Apply Gaussian blur and Otsu's thresholding\n",
    "    image_blurred = cv2.GaussianBlur(image_grayscaled, (5, 5), 0)\n",
    "    _, image_thresholded = cv2.threshold(\n",
    "        image_blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU\n",
    "    )\n",
    "    \n",
    "    # Step 2: Define a region of interest (ROI) for Hough Line Transform\n",
    "    height, width = image_grayscaled.shape\n",
    "    roi = image_grayscaled[:height // 2, :]  # Focus on the upper half of the image\n",
    "    \n",
    "    # Detect edges using Canny in the ROI\n",
    "    edges = cv2.Canny(roi, 50, 150)\n",
    "    \n",
    "    # Step 3: Use the Hough Line Transform in the ROI\n",
    "    lines = cv2.HoughLinesP(edges, 1, np.pi / 180, threshold=80, minLineLength=100, maxLineGap=50)\n",
    "    \n",
    "    horizon_y_candidates = []\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            x1, y1, x2, y2 = line[0]\n",
    "            if abs(y2 - y1) < 10:  # Consider only nearly horizontal lines\n",
    "                horizon_y_candidates.append((y1 + y2) // 2)\n",
    "    \n",
    "    # Step 4: Calculate the Otsu-based horizon line position\n",
    "    horizon_y_otsu = max(np.where(image_thresholded[height // 2:, :] == 0)[0]) + height // 2\n",
    "\n",
    "    # Step 5: Combine the methods\n",
    "    if horizon_y_candidates:\n",
    "        horizon_y_hough = int(np.mean(horizon_y_candidates))\n",
    "        horizon_y = int(0.5 * horizon_y_hough + 0.5 * horizon_y_otsu)\n",
    "    else:\n",
    "        horizon_y = horizon_y_otsu\n",
    "\n",
    "    horizon_x1 = 0\n",
    "    horizon_x2 = width - 1\n",
    "\n",
    "    return horizon_x1, horizon_x2, horizon_y, horizon_y\n",
    "\n",
    "# Load the video or image\n",
    "# video_path = 'path_to_your_video.mp4'  # Replace with your video path\n",
    "cap = cv2.VideoCapture('buoy_video.mp4')\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect the horizon line\n",
    "    try:\n",
    "        horizon_x1, horizon_x2, horizon_y1, horizon_y2 = detect_horizon_line_combined(gray_frame)\n",
    "        \n",
    "        # Draw the horizon line on the frame\n",
    "        frame_with_horizon = frame.copy()\n",
    "        cv2.line(frame_with_horizon, (horizon_x1, horizon_y1), (horizon_x2, horizon_y2), (0, 0, 255), 2)\n",
    "\n",
    "        # Concatenate the original frame and the frame with the horizon line\n",
    "        combined_frame = cv2.hconcat([frame, frame_with_horizon])\n",
    "\n",
    "        # Display the combined frame\n",
    "        cv2.imshow(\"Original and Horizon Line\", combined_frame)\n",
    "    except Exception as e:\n",
    "        print(\"Error detecting horizon:\", e)\n",
    "        continue\n",
    "\n",
    "    # Break on 'q' key press\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def detect_horizon_using_gradients(image_grayscaled):\n",
    "    \"\"\"Detect the horizon line using gradient information.\"\"\"\n",
    "    # Step 1: Apply a Gaussian blur to reduce noise\n",
    "    image_blurred = cv2.GaussianBlur(image_grayscaled, (5, 5), 0)\n",
    "\n",
    "    # Step 2: Compute the vertical gradient using the Sobel operator\n",
    "    sobel_y = cv2.Sobel(image_blurred, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    sobel_y = np.abs(sobel_y)  # Take the absolute value to focus on intensity\n",
    "\n",
    "    # Step 3: Sum the gradient magnitudes across each row\n",
    "    gradient_sums = np.sum(sobel_y, axis=1)\n",
    "\n",
    "    # Step 4: Find the row with the maximum gradient change in the upper half of the image\n",
    "    height = image_grayscaled.shape[0]\n",
    "    upper_half = gradient_sums[:height // 2]\n",
    "    horizon_y = np.argmax(upper_half)\n",
    "\n",
    "    return horizon_y\n",
    "\n",
    "# Load the video or image\n",
    "video_path = 'buoy_video.mp4'  # Replace with your video path\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Initialize variables for smoothing\n",
    "horizon_y_smoothed = None\n",
    "alpha = 0.9  # Smoothing factor (0 < alpha < 1)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect the horizon line\n",
    "    try:\n",
    "        horizon_y_current = detect_horizon_using_gradients(gray_frame)\n",
    "\n",
    "        # Apply exponential smoothing to make the horizon line smoother\n",
    "        if horizon_y_smoothed is None:\n",
    "            horizon_y_smoothed = horizon_y_current  # Initialize on the first frame\n",
    "        else:\n",
    "            horizon_y_smoothed = alpha * horizon_y_smoothed + (1 - alpha) * horizon_y_current\n",
    "\n",
    "        # Draw the smoothed horizon line on the frame\n",
    "        horizon_x1 = 0\n",
    "        horizon_x2 = frame.shape[1] - 1\n",
    "        horizon_y_smoothed_int = int(horizon_y_smoothed)\n",
    "\n",
    "        frame_with_horizon = frame.copy()\n",
    "        cv2.line(frame_with_horizon, (horizon_x1, horizon_y_smoothed_int), (horizon_x2, horizon_y_smoothed_int), (0, 0, 255), 2)\n",
    "\n",
    "        # Concatenate the original frame and the frame with the horizon line\n",
    "        combined_frame = cv2.hconcat([frame, frame_with_horizon])\n",
    "\n",
    "        # Display the combined frame\n",
    "        cv2.imshow(\"Original and Smoothed Horizon Line\", combined_frame)\n",
    "    except Exception as e:\n",
    "        print(\"Error detecting horizon:\", e)\n",
    "        continue\n",
    "\n",
    "    # Break on 'q' key press\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_initial_position(video_path):\n",
    "    \"\"\"\n",
    "    Plays the first few frames of the video slowly. When the user presses 's',\n",
    "    the video pauses, and they can select the initial position of the buoy.\n",
    "    Returns the (x, y) coordinates of the selected point and the frame number.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return None, None\n",
    "\n",
    "    initial_position = None\n",
    "    frame_number = 0\n",
    "\n",
    "    # Callback function to capture mouse click\n",
    "    def mouse_callback(event, x, y, flags, param):\n",
    "        nonlocal initial_position\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            initial_position = (x, y)\n",
    "            print(f\"Initial position selected at: {initial_position} in frame {frame_number}\")\n",
    "            cv2.destroyWindow(\"Select Initial Position\")  # Close the window after selection\n",
    "\n",
    "    # Set up window and callback\n",
    "    cv2.namedWindow(\"Select Initial Position\")\n",
    "    cv2.setMouseCallback(\"Select Initial Position\", mouse_callback)\n",
    "\n",
    "    frame_delay = 500  # Delay in milliseconds to slow down frames\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Reached the end of the video or encountered an error.\")\n",
    "            break\n",
    "\n",
    "        frame_number = int(cap.get(cv2.CAP_PROP_POS_FRAMES))  # Current frame number\n",
    "        cv2.imshow(\"Select Initial Position\", frame)\n",
    "        key = cv2.waitKey(frame_delay) & 0xFF\n",
    "\n",
    "        if key == ord('s'):  # Press 's' to select initial position\n",
    "            print(\"Press 's' detected. Click on the frame to select the initial position.\")\n",
    "            while initial_position is None:\n",
    "                cv2.waitKey(1)\n",
    "            break\n",
    "        elif key == ord('q'):  # Press 'q' to quit\n",
    "            print(\"Selection canceled.\")\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    return initial_position, frame_number\n",
    "\n",
    "def select_roi(frame, initial_position):\n",
    "    \"\"\"Zooms into the selected area and allows the user to select the ROI.\"\"\"\n",
    "    x, y = initial_position\n",
    "    zoom_scale = 2.0  # Scale factor for zooming\n",
    "    h, w = frame.shape[:2]\n",
    "\n",
    "    # Define the zoomed-in area\n",
    "    start_x = max(0, int(x - w / (2 * zoom_scale)))\n",
    "    start_y = max(0, int(y - h / (2 * zoom_scale)))\n",
    "    end_x = min(frame.shape[1], int(x + w / (2 * zoom_scale)))\n",
    "    end_y = min(frame.shape[0], int(y + h / (2 * zoom_scale)))\n",
    "\n",
    "    # Crop and resize the frame for zoom effect\n",
    "    zoomed_frame = frame[start_y:end_y, start_x:end_x]\n",
    "    zoomed_frame = cv2.resize(zoomed_frame, (w, h))\n",
    "\n",
    "    # Show the zoomed-in frame and allow ROI selection\n",
    "    roi = cv2.selectROI(\"Select ROI\", zoomed_frame, fromCenter=False, showCrosshair=True)\n",
    "    cv2.destroyWindow(\"Select ROI\")\n",
    "\n",
    "    # Calculate the bounding box in the original frame\n",
    "    roi_x, roi_y, roi_w, roi_h = roi\n",
    "    original_bbox = (start_x + roi_x, start_y + roi_y, roi_w, roi_h)\n",
    "    \n",
    "    return original_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import cv2\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2.aruco as aruco\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "def apply_motion_model(prev_gray_frame, curr_gray_frame):\n",
    "    \"\"\"Estimate motion between two grayscale frames.\"\"\"\n",
    "    flow = cv2.calcOpticalFlowFarneback(prev_gray_frame, curr_gray_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    dx, dy = np.median(flow[..., 0]), np.median(flow[..., 1])\n",
    "    return dx, dy\n",
    "\n",
    "def apply_background_subtraction(frame, background):\n",
    "    \"\"\"Applies background subtraction to reduce wave motion.\"\"\"\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    background_gray = cv2.cvtColor(background, cv2.COLOR_BGR2GRAY)\n",
    "    diff = cv2.absdiff(background_gray, frame_gray)\n",
    "    _, thresh = cv2.threshold(diff, 50, 255, cv2.THRESH_BINARY) #30 255\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "(major_ver, minor_ver, subminor_ver) = (cv2.__version__).split('.')\n",
    "def set_CSRT_Params():\n",
    "    # Don't modify\n",
    "    default_params = {\n",
    "        'padding': 3.,\n",
    "        'template_size': 200.,\n",
    "        'gsl_sigma': 1.,\n",
    "        'hog_orientations': 9.,\n",
    "        'num_hog_channels_used': 18,\n",
    "        'hog_clip': 2.0000000298023224e-01,\n",
    "        'use_hog': 1,\n",
    "        'use_color_names': 1,\n",
    "        'use_gray': 1,\n",
    "        'use_rgb': 0,\n",
    "        'window_function': 'hann',\n",
    "        'kaiser_alpha': 3.7500000000000000e+00,\n",
    "        'cheb_attenuation': 45.,\n",
    "        'filter_lr': 1.9999999552965164e-02,\n",
    "        'admm_iterations': 4,\n",
    "        'number_of_scales': 100,\n",
    "        'scale_sigma_factor': 0.25,\n",
    "        'scale_model_max_area': 512.,\n",
    "        'scale_lr': 2.5000000372529030e-02,\n",
    "        'scale_step': 1.02,\n",
    "        'use_channel_weights': 1,\n",
    "        'weights_lr': 1.9999999552965164e-02,\n",
    "        'use_segmentation': 1,\n",
    "        'histogram_bins': 16,\n",
    "        'background_ratio': 2,\n",
    "        'histogram_lr': 3.9999999105930328e-02,\n",
    "        'psr_threshold': 3.5000000149011612e-02,\n",
    "    }\n",
    "    # modify\n",
    "    params = {\n",
    "        # 'scale_lr': 0.5,\n",
    "        'number_of_scales': 33,\n",
    "        'scale_step': 1.05,\n",
    "        # 'filter_lr': 0.01,\n",
    "        # 'weights_lr': 0.01,\n",
    "        'padding': 1.5,\n",
    "        'psr_threshold': 0.05,\n",
    "        'use_channel_weights': 0\n",
    "\n",
    "    }\n",
    "    params = {**default_params, **params}\n",
    "    tracker = None\n",
    "    if int(major_ver) == 3 and 3 <= int(minor_ver) <= 4:\n",
    "        import json\n",
    "        import os\n",
    "        with open('tmp.json', 'w') as fid:\n",
    "            json.dump(params, fid)\n",
    "        fs_settings = cv2.FileStorage(\"tmp.json\", cv2.FILE_STORAGE_READ)\n",
    "        tracker = cv2.TrackerCSRT_create()\n",
    "        tracker.read(fs_settings.root())\n",
    "        os.remove('tmp.json')\n",
    "    elif int(major_ver) >= 4:\n",
    "        param_handler = cv2.TrackerCSRT_Params()\n",
    "        for key, val in params.items():\n",
    "            setattr(param_handler, key, val)\n",
    "        tracker = cv2.TrackerCSRT_create(param_handler)\n",
    "    else:\n",
    "        print(\"Cannot set parameters, using defaults\")\n",
    "        tracker = cv2.TrackerCSRT_create()\n",
    "    return tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import depthCalculation\n",
    "import importlib\n",
    "importlib.reload(depthCalculation)\n",
    "\n",
    "# Parameters\n",
    "video_path = 'stabilized_video.mp4'  # Path to the already stabilized video\n",
    "initial_position, frame_number = (611, 491),  23  # select_initial_position(video_path)\n",
    "\n",
    "# Initialize video capture using the stabilized video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Set the position to the specified frame number\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "\n",
    "# Use the first stabilized frame as the background\n",
    "success, background = cap.read()\n",
    "if not success:\n",
    "    print(\"Failed to read the background frame.\")\n",
    "    cap.release()\n",
    "    exit()\n",
    "\n",
    "# Allow user to select the ROI based on the initial position\n",
    "init_bbox = (593, 476, 37, 17)  # Predefined initial bounding box\n",
    "# tracker = cv2.TrackerCSRT_create()\n",
    "tracker = set_CSRT_Params()\n",
    "tracker.init(background, init_bbox)\n",
    "\n",
    "# Initialize variables for motion compensation\n",
    "prev_gray_frame = cv2.cvtColor(background, cv2.COLOR_BGR2GRAY)\n",
    "is_paused = False  # Control pausing\n",
    "frame_count = frame_number\n",
    "\n",
    "# Function to adjust the bounding box for a search area\n",
    "def adjust_bbox(bbox, scale=1.5):\n",
    "    (x, y, w, h) = [int(v) for v in bbox]\n",
    "    print(\"bbox\", bbox)\n",
    "    print(x,y,w,h)\n",
    "    # Expand the bounding box by the scale factor\n",
    "    return (int(x - w * (scale - 1) / 2), int(y - h * (scale - 1) / 2),\n",
    "            int(w * scale), int(h * scale))\n",
    "\n",
    "# Begin tracking loop\n",
    "while True:\n",
    "    if not is_paused:\n",
    "        # Read the next frame from the video\n",
    "        ret, frame = cap.read()\n",
    "        frame_count += 1\n",
    "        if not ret:\n",
    "            break  # Exit if no frames are left\n",
    "\n",
    "        # If the buoy is lost and outside the frames to be skipped\n",
    "        if (frame_count >= 45 and frame_count <= 58) or (frame_count >= 138 and frame_count <= 145):\n",
    "            success = False  # Mark as lost but do not search again\n",
    "\n",
    "        # Update the tracker\n",
    "        success, bbox = tracker.update(frame)\n",
    "\n",
    "        # if not success:\n",
    "        #     # If tracking fails and it's not within frames 45-58 or 138-145\n",
    "        #     if (frame_count < 45 or frame_count > 58) and (frame_count < 138 or frame_count > 145):\n",
    "        #         print(\"LOST\")\n",
    "        #         search_bbox = adjust_bbox(init_bbox, scale=5)  # Expand search area\n",
    "        #         cv2.rectangle(frame, (search_bbox[0], search_bbox[1]),\n",
    "        #                       (search_bbox[0] + search_bbox[2], search_bbox[1] + search_bbox[3]),\n",
    "        #                       (255, 0, 0), 2)  # Draw search area rectangle\n",
    "        #         print(search_bbox)\n",
    "                # Here you can implement additional logic to search within search_bbox if desired\n",
    "                # bbox = cv2.selectROI(\"Select ROI\", frame, fromCenter=False, showCrosshair=True)\n",
    "                # tracker.init(frame, bbox)  # Reinitialize the tracker\n",
    "\n",
    "        # Convert the current frame to grayscale for motion model\n",
    "        curr_gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Estimate motion using motion model\n",
    "        dx, dy = apply_motion_model(prev_gray_frame, curr_gray_frame)\n",
    "        motion_compensated_frame = cv2.warpAffine(frame, np.float32([[1, 0, -dx], [0, 1, -dy]]), \n",
    "                                                                   (frame.shape[1], frame.shape[0]))  # Apply motion compensation\n",
    "\n",
    "        # Update the tracker\n",
    "        success, bbox = tracker.update(motion_compensated_frame)\n",
    "\n",
    "        if success:\n",
    "            (x, y, w, h) = [int(v) for v in bbox]\n",
    "            # Dark gray rectangle (2 pixels)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (200, 65, 55), 2)\n",
    "            # Create a light gray textbox for the distance text\n",
    "            x_coor = x + w // 2\n",
    "            y_coor = y + h // 2\n",
    "            distance_buoy = depthCalculation.detect_horizontal_lines_in_video(frame, x_coor, y_coor)\n",
    "\n",
    "            text = \"Distance \" + str(round(distance_buoy,4)) + \"m\"\n",
    "            text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]\n",
    "            text_x = x\n",
    "            text_y = y - 10\n",
    "            cv2.rectangle(frame, (text_x, text_y - text_size[1] - 5), \n",
    "                             (text_x + text_size[0], text_y + 5), (200, 200, 200), -1)\n",
    "            cv2.putText(frame, text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (50, 50, 50), 1)\n",
    "        else:\n",
    "            # When lost, do not show bounding box or text during frames 45-58\n",
    "            # if not (frame_count >= 45 and frame_count <= 58):\n",
    "            cv2.putText(frame, \"Buoy Lost\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "        # Dark gray frame count text with prefix\n",
    "        frame_count_text = f\"Frame {frame_count}\"\n",
    "        cv2.putText(frame, frame_count_text, (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (50, 50, 50), 2)\n",
    "\n",
    "        # Show the frame\n",
    "        cv2.imshow(\"Motion free frame\", motion_compensated_frame)\n",
    "        cv2.imshow(\"Buoy Tracking\", frame)\n",
    "\n",
    "        # Update the previous gray frame\n",
    "        prev_gray_frame = curr_gray_frame\n",
    "\n",
    "    # Check for key presses\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    elif key == ord(' '):  # Space bar to pause/resume\n",
    "        is_paused = not is_paused  # Toggle pause\n",
    "\n",
    "# Release video capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
