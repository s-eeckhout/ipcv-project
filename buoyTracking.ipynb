{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import cv2\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2.aruco as aruco\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "video_path = 'buoy_video.mp4'  # Replace with your video path\n",
    "output_stabilized = 'stabilized_video.mp4'  # Output path for stabilized video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stabilization\n",
    "\n",
    "by: https://github.com/krutikabapat/Video-Stabilization-using-OpenCV/blob/master/video_stabilization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "SMOOTHING_RADIUS = 50\n",
    "\n",
    "def movingAverage(curve, radius): \n",
    "    window_size = 2 * radius + 1\n",
    "    f = np.ones(window_size) / window_size \n",
    "    curve_pad = np.lib.pad(curve, (radius, radius), 'edge') \n",
    "    curve_smoothed = np.convolve(curve_pad, f, mode='same') \n",
    "    curve_smoothed = curve_smoothed[radius:-radius]\n",
    "    return curve_smoothed \n",
    "\n",
    "def smooth(trajectory): \n",
    "    smoothed_trajectory = np.copy(trajectory) \n",
    "    for i in range(3):\n",
    "        smoothed_trajectory[:, i] = movingAverage(trajectory[:, i], radius=SMOOTHING_RADIUS)\n",
    "    return smoothed_trajectory\n",
    "\n",
    "def fixBorder(frame):\n",
    "    s = frame.shape\n",
    "    T = cv2.getRotationMatrix2D((s[1]/2, s[0]/2), 0, 1.04)\n",
    "    frame = cv2.warpAffine(frame, T, (s[1], s[0]))\n",
    "    return frame\n",
    "\n",
    "# Read input video\n",
    "cp = cv2.VideoCapture(video_path)\n",
    "\n",
    "n_frames = int(cp.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(n_frames)\n",
    "\n",
    "width = int(cp.get(cv2.CAP_PROP_FRAME_WIDTH)) \n",
    "height = int(cp.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "print(\"Width:\", width)\n",
    "print(\"Height:\", height)\n",
    "\n",
    "fps = cp.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "out = cv2.VideoWriter(output_stabilized, fourcc, fps, (width, height))\n",
    "\n",
    "_, prev = cp.read()\n",
    "prev_gray = cv2.cvtColor(prev, cv2.COLOR_BGR2GRAY)\n",
    "transforms = np.zeros((n_frames - 1, 3), np.float32) \n",
    "\n",
    "for i in range(n_frames - 2):\n",
    "    prev_pts = cv2.goodFeaturesToTrack(prev_gray, maxCorners=200, qualityLevel=0.01, minDistance=30, blockSize=3)\n",
    "    succ, curr = cp.read()\n",
    "\n",
    "    if not succ:\n",
    "        break\n",
    "\n",
    "    curr_gray = cv2.cvtColor(curr, cv2.COLOR_BGR2GRAY)\n",
    "    curr_pts, status, err = cv2.calcOpticalFlowPyrLK(prev_gray, curr_gray, prev_pts, None)\n",
    "\n",
    "    idx = np.where(status == 1)[0]\n",
    "    prev_pts = prev_pts[idx]\n",
    "    curr_pts = curr_pts[idx]\n",
    "    assert prev_pts.shape == curr_pts.shape \n",
    "\n",
    "    m, inliers = cv2.estimateAffine2D(prev_pts, curr_pts)\n",
    "    if m is None:\n",
    "        print(\"Could not estimate affine transformation\")\n",
    "        continue\n",
    "\n",
    "    dx = m[0, 2]\n",
    "    dy = m[1, 2]\n",
    "    da = np.arctan2(m[1, 0], m[0, 0])\n",
    "    transforms[i] = [dx, dy, da] \n",
    "    prev_gray = curr_gray\n",
    "\n",
    "trajectory = np.cumsum(transforms, axis=0) \n",
    "smoothed_trajectory = smooth(trajectory)\n",
    "difference = smoothed_trajectory - trajectory\n",
    "transforms_smooth = transforms + difference\n",
    "\n",
    "cp.set(cv2.CAP_PROP_POS_FRAMES, 0) \n",
    "for i in range(n_frames - 2):\n",
    "    success, frame = cp.read() \n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    dx = transforms_smooth[i, 0]\n",
    "    dy = transforms_smooth[i, 1]\n",
    "    da = transforms_smooth[i, 2]\n",
    "\n",
    "    m = np.zeros((2, 3), np.float32)\n",
    "    m[0, 0] = np.cos(da)\n",
    "    m[0, 1] = -np.sin(da)\n",
    "    m[1, 0] = np.sin(da)\n",
    "    m[1, 1] = np.cos(da)\n",
    "    m[0, 2] = dx\n",
    "    m[1, 2] = dy\n",
    "\n",
    "    frame_stabilized = cv2.warpAffine(frame, m, (width, height))\n",
    "    frame_stabilized = fixBorder(frame_stabilized) \n",
    "\n",
    "    frame_out = cv2.hconcat([frame, frame_stabilized])\n",
    "    \n",
    "    if frame_out.shape[1] > 1920: \n",
    "        frame_out = cv2.resize(frame_out, (int(frame_out.shape[1] / 2), int(frame_out.shape[0] / 2)))\n",
    "    \n",
    "    cv2.imshow(\"Before and After\", frame_out)\n",
    "    cv2.waitKey(10)\n",
    "    out.write(frame_stabilized)\n",
    "\n",
    "cp.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_initial_position(video_path):\n",
    "    \"\"\"\n",
    "    Plays the first few frames of the video slowly. When the user presses 's',\n",
    "    the video pauses, and they can select the initial position of the buoy.\n",
    "    Returns the (x, y) coordinates of the selected point and the frame number.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return None, None\n",
    "\n",
    "    initial_position = None\n",
    "    frame_number = 0\n",
    "\n",
    "    # Callback function to capture mouse click\n",
    "    def mouse_callback(event, x, y, flags, param):\n",
    "        nonlocal initial_position\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            initial_position = (x, y)\n",
    "            print(f\"Initial position selected at: {initial_position} in frame {frame_number}\")\n",
    "            cv2.destroyWindow(\"Select Initial Position\")  # Close the window after selection\n",
    "\n",
    "    # Set up window and callback\n",
    "    cv2.namedWindow(\"Select Initial Position\")\n",
    "    cv2.setMouseCallback(\"Select Initial Position\", mouse_callback)\n",
    "\n",
    "    frame_delay = 500  # Delay in milliseconds to slow down frames\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Reached the end of the video or encountered an error.\")\n",
    "            break\n",
    "\n",
    "        frame_number = int(cap.get(cv2.CAP_PROP_POS_FRAMES))  # Current frame number\n",
    "        cv2.imshow(\"Select Initial Position\", frame)\n",
    "        key = cv2.waitKey(frame_delay) & 0xFF\n",
    "\n",
    "        if key == ord('s'):  # Press 's' to select initial position\n",
    "            print(\"Press 's' detected. Click on the frame to select the initial position.\")\n",
    "            while initial_position is None:\n",
    "                cv2.waitKey(1)\n",
    "            break\n",
    "        elif key == ord('q'):  # Press 'q' to quit\n",
    "            print(\"Selection canceled.\")\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    return initial_position, frame_number\n",
    "\n",
    "def select_roi(frame, initial_position):\n",
    "    \"\"\"Zooms into the selected area and allows the user to select the ROI.\"\"\"\n",
    "    x, y = initial_position\n",
    "    zoom_scale = 2.0  # Scale factor for zooming\n",
    "    h, w = frame.shape[:2]\n",
    "\n",
    "    # Define the zoomed-in area\n",
    "    start_x = max(0, int(x - w / (2 * zoom_scale)))\n",
    "    start_y = max(0, int(y - h / (2 * zoom_scale)))\n",
    "    end_x = min(frame.shape[1], int(x + w / (2 * zoom_scale)))\n",
    "    end_y = min(frame.shape[0], int(y + h / (2 * zoom_scale)))\n",
    "\n",
    "    # Crop and resize the frame for zoom effect\n",
    "    zoomed_frame = frame[start_y:end_y, start_x:end_x]\n",
    "    zoomed_frame = cv2.resize(zoomed_frame, (w, h))\n",
    "\n",
    "    # Show the zoomed-in frame and allow ROI selection\n",
    "    roi = cv2.selectROI(\"Select ROI\", zoomed_frame, fromCenter=False, showCrosshair=True)\n",
    "    cv2.destroyWindow(\"Select ROI\")\n",
    "\n",
    "    # Calculate the bounding box in the original frame\n",
    "    roi_x, roi_y, roi_w, roi_h = roi\n",
    "    original_bbox = (start_x + roi_x, start_y + roi_y, roi_w, roi_h)\n",
    "    \n",
    "    return original_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_motion_model(prev_gray_frame, curr_gray_frame):\n",
    "    \"\"\"Estimate motion between two grayscale frames.\"\"\"\n",
    "    flow = cv2.calcOpticalFlowFarneback(prev_gray_frame, curr_gray_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    dx, dy = np.median(flow[..., 0]), np.median(flow[..., 1])\n",
    "    return dx, dy\n",
    "\n",
    "def apply_background_subtraction(frame, background):\n",
    "    \"\"\"Applies background subtraction to reduce wave motion.\"\"\"\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    background_gray = cv2.cvtColor(background, cv2.COLOR_BGR2GRAY)\n",
    "    diff = cv2.absdiff(background_gray, frame_gray)\n",
    "    _, thresh = cv2.threshold(diff, 30, 255, cv2.THRESH_BINARY)\n",
    "    return thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(593, 476, 37, 17)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m curr_gray_frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Estimate motion using motion model\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m dx, dy \u001b[38;5;241m=\u001b[39m \u001b[43mapply_motion_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_gray_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurr_gray_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Apply motion compensation to the current frame\u001b[39;00m\n\u001b[1;32m     46\u001b[0m motion_compensated_frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mwarpAffine(frame, np\u001b[38;5;241m.\u001b[39mfloat32([[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39mdx], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39mdy]]), (frame\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], frame\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m, in \u001b[0;36mapply_motion_model\u001b[0;34m(prev_gray_frame, curr_gray_frame)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_motion_model\u001b[39m(prev_gray_frame, curr_gray_frame):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Estimate motion between two grayscale frames.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     flow \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalcOpticalFlowFarneback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_gray_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurr_gray_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     dx, dy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmedian(flow[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m]), np\u001b[38;5;241m.\u001b[39mmedian(flow[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dx, dy\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "video_path = 'stabilized_video.mp4'  # Path to the already stabilized video\n",
    "initial_position, frame_number = (611, 491),  23 # select_initial_position(video_path)\n",
    "\n",
    "# Initialize video capture using the stabilized video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Set the position to the specified frame number\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "\n",
    "# Use the first stabilized frame as the background\n",
    "success, background = cap.read()\n",
    "if not success:\n",
    "    print(\"Failed to read the background frame.\")\n",
    "    cap.release()\n",
    "    exit()\n",
    "\n",
    "# Allow user to select the ROI based on the initial position\n",
    "# bbox = select_roi(background, initial_position)\n",
    "bbox = (593, 476, 37, 17) # cv2.selectROI(\"Select ROI\", background, fromCenter=False, showCrosshair=True)\n",
    "print(bbox)\n",
    "\n",
    "# Initialize the CSRT tracker with the first stabilized frame\n",
    "tracker = cv2.TrackerCSRT_create()\n",
    "tracker.init(background, bbox)\n",
    "\n",
    "# Initialize variables for motion compensation\n",
    "prev_gray_frame = cv2.cvtColor(background, cv2.COLOR_BGR2GRAY)\n",
    "is_paused = False # Control pausing\n",
    "\n",
    "# Begin tracking loop\n",
    "while True:\n",
    "    if not is_paused:\n",
    "        # Read the next frame from the video\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break  # Exit if no frames are left\n",
    "\n",
    "        # Convert the current frame to grayscale for motion model\n",
    "        curr_gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Estimate motion using motion model\n",
    "        dx, dy = apply_motion_model(prev_gray_frame, curr_gray_frame)\n",
    "\n",
    "        # Apply motion compensation to the current frame\n",
    "        motion_compensated_frame = cv2.warpAffine(frame, np.float32([[1, 0, -dx], [0, 1, -dy]]), (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "        # Apply background subtraction to reduce wave motion\n",
    "        motion_free_frame = apply_background_subtraction(motion_compensated_frame, background)\n",
    "\n",
    "        # Update the tracker\n",
    "        success, bbox = tracker.update(motion_free_frame)\n",
    "\n",
    "        if success:\n",
    "            (x, y, w, h) = [int(v) for v in bbox]\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, \"Buoy\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        else:\n",
    "            cv2.putText(frame, \"Lost\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "        cv2.circle(frame, initial_position, radius=5, color=(0, 0, 255), thickness=-1)\n",
    "\n",
    "        # Show the frame\n",
    "        cv2.imshow(\"Buoy Tracking\", frame)\n",
    "\n",
    "        # Update the previous gray frame\n",
    "        prev_gray_frame = curr_gray_frame\n",
    "\n",
    "    # Check for key presses\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    elif key == ord(' '):  # Space bar to pause/resume\n",
    "        is_paused = not is_paused  # Toggle pause\n",
    "\n",
    "# Release video capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
